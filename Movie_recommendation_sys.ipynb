{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kij74y6Lao34tLbdFzFZMnyUUkyVFSBi",
      "authorship_tag": "ABX9TyPOo6WEwyQooPTYxe2I5ACc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yugamjayant/sept_2023_jh/blob/main/Movie_recommendation_sys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL based movie recommendation system\n",
        "\n",
        "Movie lens Dataset(small) :https://grouplens.org/datasets/movielens/latest/\n",
        "\n",
        "We will build a reccomender system using content filtering algorithm"
      ],
      "metadata": {
        "id": "uw5Sw1iBZBQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqilaIhC706e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "import re\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "links_rawds = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/ml-latest-small/links.csv\")\n",
        "movies_rawds = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/ml-latest-small/movies.csv')\n",
        "ratings_rawds = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/ml-latest-small/ratings.csv')\n",
        "tags_rawds = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/ml-latest-small/tags.csv')"
      ],
      "metadata": {
        "id": "WRftlebIyTB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "4d8MsCz4DtmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets prepare data that can be fed to the model\n",
        "#Lets start by preaparing User side of data\n",
        "\n",
        "movie_genres = copy.deepcopy(movies_rawds)\n",
        "\n",
        "all_genres = set()\n",
        "for index, row in movies_rawds.iterrows():\n",
        "  all_genres.update(row['genres'].split('|'))\n",
        "\n",
        "#Remove \"(no genres listed)\" from genres list\n",
        "all_genres.discard('(no genres listed)')\n",
        "\n",
        "\n",
        "#Adding new columns, denoting presence of each genre for each movie\n",
        "\n",
        "for genre in all_genres:\n",
        "  movie_genres[genre] = movie_genres['genres'].str.contains(genre)\n",
        "\n",
        "\n",
        "#Merging the dataframes, rating rewards dataframe and movie genres dataframe\n",
        "\n",
        "ratings_movie_genres = pd.merge(left = ratings_rawds[['userId', 'movieId', 'rating']],\n",
        "                                right = movie_genres,\n",
        "                                on = 'movieId',\n",
        "                                how = 'left')\n",
        "\n",
        "\n",
        "#Function that renames columns, it helps in assigning new column names after aggregatte function\n",
        "def create_col_names(agg_dict, agg_col):\n",
        "  col_names = [agg_col]\n",
        "  for key in agg_dict.keys():\n",
        "    value = agg_dict[key]\n",
        "    if type(value) is list:\n",
        "      for value_1 in value:\n",
        "        col_names.append(key+'_'+value_1)\n",
        "    elif type(value) != str:\n",
        "      col_names.append(key+'_'+'cfunc')\n",
        "    else:\n",
        "      col_names.append(key+'_'+value)\n",
        "\n",
        "  return col_names\n",
        "\n",
        "\n",
        "#This function calculates genre wise avr ratings\n",
        "def genre_wise_avg_rating(array_temp):\n",
        "\n",
        "  sum_ratings = array_temp.sum()\n",
        "  #Non zero ratings count\n",
        "  array_temp_summary = array_temp.value_counts()\n",
        "  non_zero_ratings_count = array_temp_summary.sum()\n",
        "\n",
        "  if 0 in array_temp_summary.index:\n",
        "    non_zero_ratings_count = array_temp_summary.sum() - array_temp_summary[0]\n",
        "\n",
        "  return sum_ratings/non_zero_ratings_count\n",
        "\n",
        "\n",
        "\n",
        "#Adding new columns, denoting presence of each genre for each movie\n",
        "\n",
        "for genre in all_genres:\n",
        "  movie_genres[genre] = movie_genres['genres'].str.contains(genre)\n",
        "\n",
        "\n",
        "\n",
        "#Merging the dataframes, rating rewards dataframe and movie genres dataframe\n",
        "\n",
        "ratings_movie_genres = pd.merge(left = ratings_rawds[['userId', 'movieId', 'rating']],\n",
        "                                right = movie_genres,\n",
        "                                on = 'movieId',\n",
        "                                how = 'left')\n",
        "\n",
        "\n",
        "#Function that renames columns, it helps in assigning new column names after aggregatte function\n",
        "def create_col_names(agg_dict, agg_col):\n",
        "  col_names = [agg_col]\n",
        "  for key in agg_dict.keys():\n",
        "    value = agg_dict[key]\n",
        "    if type(value) is list:\n",
        "      for value_1 in value:\n",
        "        col_names.append(key+'_'+value_1)\n",
        "    elif type(value) != str:\n",
        "      col_names.append(key+'_'+'cfunc')\n",
        "    else:\n",
        "      col_names.append(key+'_'+value)\n",
        "\n",
        "  return col_names\n",
        "\n",
        "\n",
        "global user_genre_ratings_array\n",
        "\n",
        "#This function calculates genre wise avr ratings\n",
        "def genre_wise_avg_rating(array_temp):\n",
        "  global user_genre_ratings_array\n",
        "  user_genre_ratings_array = array_temp\n",
        "  sum_ratings = array_temp.sum()\n",
        "  #Non zero ratings count\n",
        "  array_temp_summary = array_temp.value_counts()\n",
        "  non_zero_ratings_count = array_temp_summary.sum()\n",
        "\n",
        "  if 0 in array_temp_summary.index:\n",
        "    non_zero_ratings_count = array_temp_summary.sum() - array_temp_summary[0]\n",
        "\n",
        "  return sum_ratings/non_zero_ratings_count\n",
        "\n",
        "#Adding rating of each movie to genre column, to facilitate averages in thenext step\n",
        "for genre in all_genres:\n",
        "  ratings_movie_genres[genre] = ratings_movie_genres[genre].astype(float) * ratings_movie_genres['rating']\n",
        "\n",
        "\n",
        "#Aggregating the dataframe to compute average ratings for each genre\n",
        "movie_genre_df_agg_dict = {'rating':['mean', 'sum'], 'movieId':'count', 'War': genre_wise_avg_rating, 'Adventure': genre_wise_avg_rating, 'Romance': genre_wise_avg_rating, 'Musical': genre_wise_avg_rating, 'Western': genre_wise_avg_rating, 'Fantasy': genre_wise_avg_rating, 'Children': genre_wise_avg_rating, 'Sci-Fi': genre_wise_avg_rating, 'Drama': genre_wise_avg_rating, 'Crime': genre_wise_avg_rating, 'Mystery': genre_wise_avg_rating, 'Horror': genre_wise_avg_rating, 'Documentary': genre_wise_avg_rating, 'Action': genre_wise_avg_rating, 'Comedy': genre_wise_avg_rating, 'Thriller': genre_wise_avg_rating, 'Animation': genre_wise_avg_rating, 'Film-Noir': genre_wise_avg_rating, 'IMAX': genre_wise_avg_rating}\n",
        "\n",
        "ratings_movie_genres_int = ratings_movie_genres.groupby('userId').aggregate(movie_genre_df_agg_dict).reset_index()\n",
        "\n",
        "ratings_movie_genres_int.columns = create_col_names(movie_genre_df_agg_dict, 'userId')\n",
        "\n",
        "ratings_movie_genres_int.fillna(0, inplace = True)\n",
        "\n",
        "\n",
        "#Expanding the df such that we our number of rows matches the number of ratings\n",
        "ratings_movie_genres_int2 = pd.merge(ratings_rawds[['userId']], ratings_movie_genres_int, on = 'userId', how = 'left')\n",
        "\n",
        "\n",
        "#Excluded few unnecessary columns\n",
        "user_ratings_df = ratings_movie_genres_int2[['userId','movieId_count', 'rating_mean', 'Action_cfunc', 'Western_cfunc', 'Sci-Fi_cfunc', 'IMAX_cfunc', 'Drama_cfunc',\n",
        "       'Thriller_cfunc', 'Adventure_cfunc', 'Animation_cfunc', 'Musical_cfunc', 'Mystery_cfunc', 'Fantasy_cfunc',\n",
        "       'Children_cfunc', 'Film-Noir_cfunc', 'Romance_cfunc', 'Crime_cfunc', 'War_cfunc', 'Documentary_cfunc',\n",
        "       'Horror_cfunc', 'Comedy_cfunc']]\n",
        "\n",
        "\n",
        "#Now Preparing item side of dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#We needed year in the dataframe, thus the following function extracts year from movie title\n",
        "\n",
        "def extract_year(title):\n",
        "  try:\n",
        "    year = re.search(\"\\(([0-9]{4})\\)\", title).group().strip('()')\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"Error: \", e)\n",
        "    year = 0\n",
        "\n",
        "  return int(year)\n",
        "\n",
        "\n",
        "movie_genres['year'] = movie_genres['title'].apply(lambda x: extract_year(x))\n",
        "\n",
        "\n",
        "#Expanding the df such that we our number of rows matches the number of ratings\n",
        "\n",
        "movie_genres_ratings = pd.merge(ratings_rawds[['movieId']], movie_genres, on = 'movieId', how = 'left')\n",
        "\n",
        "for genre in all_genres:\n",
        "  movie_genres_ratings[genre] = movie_genres_ratings[genre].astype(int)\n",
        "\n",
        "\n",
        "movie_genres_ratings.drop(columns = ['genres'], inplace = True)\n",
        "\n",
        "#calculating avg ratings for each movie\n",
        "avg_ratings_movie_wise = ratings_rawds.groupby('movieId').aggregate({'rating':'mean'}).reset_index().rename(columns = {'rating':'mean_ratings'})\n",
        "\n",
        "#Adding avg rating(per movie) column to movie_genres_ratings\n",
        "movie_genres_ratings = pd.merge(movie_genres_ratings, avg_ratings_movie_wise[['movieId','mean_ratings']], on = 'movieId', how = 'left')\n",
        "\n",
        "item_ratings_df = movie_genres_ratings[['movieId', 'year','mean_ratings','Action', 'Western', 'Sci-Fi', 'IMAX', 'Drama',\n",
        "       'Thriller', 'Adventure', 'Animation', 'Musical', 'Mystery', 'Fantasy',\n",
        "       'Children', 'Film-Noir', 'Romance', 'Crime', 'War', 'Documentary',\n",
        "       'Horror', 'Comedy']]\n",
        "\n"
      ],
      "metadata": {
        "id": "cKqU4FyT1T0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating user and item numpy arrays, here we have sliced the dataframe to remove the index column\n",
        "\n",
        "item_train = item_ratings_df.to_numpy()[:,1:]\n",
        "user_train = user_ratings_df.to_numpy()[:,1:]\n",
        "y_train = ratings_rawds['rating'].to_numpy()"
      ],
      "metadata": {
        "id": "lso_PtNdD9Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale training data\n",
        "item_train_unscaled = item_train\n",
        "user_train_unscaled = user_train\n",
        "y_train_unscaled    = y_train\n",
        "\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
        "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))\n",
        "\n",
        "#np.allclose, Returns True if two arrays are element-wise equal within a tolerance."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qeTmi54VvcS",
        "outputId": "12f1c90f-f4b5-4d1b-d675-b7b1ff8d3fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test train split\n",
        "\n",
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
        "print(f\"movie/item training data shape: {item_train.shape}\")\n",
        "print(f\"movie/item test data shape: {item_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD5s5FDYY9kD",
        "outputId": "15ee5105-97a8-49cc-b147-5ddd42e4e915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movie/item training data shape: (80668, 21)\n",
            "movie/item test data shape: (20168, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model Neural Network architecture for content-based filtering"
      ],
      "metadata": {
        "id": "14BgMS1Kbv4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_outputs = 32\n",
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    ### START CODE HERE ###\n",
        "      tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "      tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "      tf.keras.layers.Dense(num_outputs, activation = 'linear')\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "    ### START CODE HERE ###\n",
        "      tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "      tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "      tf.keras.layers.Dense(num_outputs, activation = 'linear')\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "])\n",
        "\n",
        "# create the user input and point to the base network\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "# create the item input and point to the base network\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "# compute the dot product of the two vectors vu and vm\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gOZts2GZLkd",
        "outputId": "db7b86bd-4faf-4057-d731-98d57ea7b0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 18)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 20)]                 0         []                            \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)   (None, 32)                   41888     ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)   (None, 32)                   42400     ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.l2_normalize (TFOp  (None, 32)                   0         ['sequential_2[0][0]']        \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.math.l2_normalize_1 (TF  (None, 32)                   0         ['sequential_3[0][0]']        \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 1)                    0         ['tf.math.l2_normalize[0][0]',\n",
            "                                                                     'tf.math.l2_normalize_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 84288 (329.25 KB)\n",
            "Trainable params: 84288 (329.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will use a mean squared error loss and an Adam optimizer.\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=cost_fn)"
      ],
      "metadata": {
        "id": "GXSnbTpobELl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 1  # start of columns to use in training, items\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh_aPQGTcIn7",
        "outputId": "3b785273-6586-4668-bbea-e366b352ac3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2521/2521 [==============================] - 13s 4ms/step - loss: 0.1356\n",
            "Epoch 2/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1294\n",
            "Epoch 3/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1267\n",
            "Epoch 4/30\n",
            "2521/2521 [==============================] - 9s 3ms/step - loss: 0.1251\n",
            "Epoch 5/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1237\n",
            "Epoch 6/30\n",
            "2521/2521 [==============================] - 10s 4ms/step - loss: 0.1224\n",
            "Epoch 7/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1208\n",
            "Epoch 8/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1194\n",
            "Epoch 9/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1186\n",
            "Epoch 10/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1179\n",
            "Epoch 11/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1173\n",
            "Epoch 12/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1169\n",
            "Epoch 13/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1164\n",
            "Epoch 14/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1160\n",
            "Epoch 15/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1156\n",
            "Epoch 16/30\n",
            "2521/2521 [==============================] - 11s 4ms/step - loss: 0.1151\n",
            "Epoch 17/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1147\n",
            "Epoch 18/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1140\n",
            "Epoch 19/30\n",
            "2521/2521 [==============================] - 9s 3ms/step - loss: 0.1136\n",
            "Epoch 20/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1130\n",
            "Epoch 21/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1126\n",
            "Epoch 22/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1123\n",
            "Epoch 23/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1118\n",
            "Epoch 24/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1114\n",
            "Epoch 25/30\n",
            "2521/2521 [==============================] - 8s 3ms/step - loss: 0.1110\n",
            "Epoch 26/30\n",
            "2521/2521 [==============================] - 10s 4ms/step - loss: 0.1110\n",
            "Epoch 27/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1106\n",
            "Epoch 28/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1103\n",
            "Epoch 29/30\n",
            "2521/2521 [==============================] - 7s 3ms/step - loss: 0.1100\n",
            "Epoch 30/30\n",
            "2521/2521 [==============================] - 9s 4ms/step - loss: 0.1098\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d2cdbfabdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model to determine loss on the test data.\n",
        "\n",
        "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6LBRIsqceUz",
        "outputId": "ecf6fd43-363e-4be6-fd6c-1fd3e1064fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "631/631 [==============================] - 2s 3ms/step - loss: 0.1179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11787594109773636"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'll take user parameters for our target users to churn out predictions"
      ],
      "metadata": {
        "id": "fG6QYPMKcDft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions for a new user\n",
        "#First, we'll create a new user and have the model suggest movies for that user.\n",
        "\n",
        "new_user_id = 5000\n",
        "new_rating_ave = 0.0\n",
        "new_Action = 1\n",
        "new_Western = 2\n",
        "new_Sci_Fi = 4\n",
        "new_IMAX = 0\n",
        "new_Drama = 2\n",
        "new_Thriller = 2\n",
        "new_Adventure = 2\n",
        "new_Animation = 0\n",
        "new_Musical = 0\n",
        "new_Mystery = 1\n",
        "new_Fantasy = 1\n",
        "new_Children = 0\n",
        "new_Film_Noir = 0\n",
        "new_Romance = 0\n",
        "new_Crime = 1\n",
        "new_War = 0\n",
        "new_Documentary = 4\n",
        "new_Horror = 0\n",
        "new_Comedy = 1\n",
        "new_rating_count = 3\n",
        "\n",
        "\n",
        "# 'Action', 'Western', 'Sci-Fi', 'IMAX', 'Drama',\n",
        "#        'Thriller', 'Adventure', 'Animation', 'Musical', 'Mystery', 'Fantasy',\n",
        "#        'Children', 'Film-Noir', 'Romance', 'Crime', 'War', 'Documentary',\n",
        "#        'Horror', 'Comedy'\n",
        "\n",
        "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
        "                      new_Action, new_Western, new_Sci_Fi, new_IMAX, new_Drama,\n",
        "                      new_Thriller, new_Adventure, new_Animation, new_Musical, new_Mystery, new_Fantasy,\n",
        "                      new_Children, new_Film_Noir, new_Romance, new_Crime, new_War, new_Documentary,\n",
        "                      new_Horror, new_Comedy]])"
      ],
      "metadata": {
        "id": "ONk0TReoduxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_pred_movies(y_p, item, movie_dict, maxcount=10):\n",
        "    \"\"\" print results of prediction of a new user. inputs are expected to be in\n",
        "        sorted order, unscaled. \"\"\"\n",
        "    count = 0\n",
        "    disp = [[\"y_p\", \"movie id\", \"rating ave\", \"title\", \"genres\"]]\n",
        "\n",
        "    for i in range(0, y_p.shape[0]):\n",
        "        if count == maxcount:\n",
        "            break\n",
        "        count += 1\n",
        "        movie_id = item[i, 0].astype(int).astype(str)\n",
        "        disp.append([np.around(y_p[i, 0], 1), item[i, 0].astype(int), np.around(item[i, 2].astype(float), 1),\n",
        "                     movie_dict[movie_id]['title'], movie_dict[movie_id]['genres']])\n",
        "\n",
        "    table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "    return table"
      ],
      "metadata": {
        "id": "sINnp1dTQnz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie reccomendations for our new user"
      ],
      "metadata": {
        "id": "Yzr7Z-v8c5Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#9742 is number of movies\n",
        "# generate and replicate the user vector to match the number movies in the data set.\n",
        "user_vecs = np.tile(user_vec[:,1:], (9742, 1))\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction\n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "\n",
        "display(HTML(print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "PWif0TdbgxlT",
        "outputId": "58a74771-3433-4244-b4ab-cb63b9bb4270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "305/305 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">  y_p</th><th style=\"text-align: right;\">  movie id</th><th style=\"text-align: right;\">  rating ave</th><th>title                                               </th><th>genres                  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">  3.8</td><td style=\"text-align: right;\">      2014</td><td style=\"text-align: right;\">           1</td><td>Freaky Friday (1977)                                </td><td>Children|Comedy|Fantasy </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.8</td><td style=\"text-align: right;\">      1980</td><td style=\"text-align: right;\">           1</td><td>Friday the 13th Part VII: The New Blood (1988)      </td><td>Horror                  </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      1972</td><td style=\"text-align: right;\">           1</td><td>Nightmare on Elm Street 5: The Dream Child, A (1989)</td><td>Horror                  </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      1975</td><td style=\"text-align: right;\">           1</td><td>Friday the 13th Part 2 (1981)                       </td><td>Horror                  </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      1951</td><td style=\"text-align: right;\">           0</td><td>Oliver! (1968)                                      </td><td>Drama|Musical           </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      1957</td><td style=\"text-align: right;\">           0</td><td>Chariots of Fire (1981)                             </td><td>Drama                   </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      2016</td><td style=\"text-align: right;\">           1</td><td>Apple Dumpling Gang Rides Again, The (1979)         </td><td>Children|Comedy|Western </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      2017</td><td style=\"text-align: right;\">           1</td><td>Babes in Toyland (1961)                             </td><td>Children|Fantasy|Musical</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      2007</td><td style=\"text-align: right;\">           1</td><td>Polish Wedding (1998)                               </td><td>Comedy                  </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  3.7</td><td style=\"text-align: right;\">      1997</td><td style=\"text-align: right;\">           1</td><td>Exorcist, The (1973)                                </td><td>Horror|Mystery          </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lez49rmFcB8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}