{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-19T10:22:41.143101Z","iopub.execute_input":"2023-09-19T10:22:41.144021Z","iopub.status.idle":"2023-09-19T10:22:41.699825Z","shell.execute_reply.started":"2023-09-19T10:22:41.143941Z","shell.execute_reply":"2023-09-19T10:22:41.698533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets Try n Implement linear regrssion on this dataset, without scikitlearn**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport copy, math","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:45.390281Z","iopub.execute_input":"2023-09-19T10:22:45.391149Z","iopub.status.idle":"2023-09-19T10:22:45.397581Z","shell.execute_reply.started":"2023-09-19T10:22:45.391049Z","shell.execute_reply":"2023-09-19T10:22:45.396190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = pd.read_csv(\"/kaggle/input/medical-cost-dataset/medical_cost.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:45.891750Z","iopub.execute_input":"2023-09-19T10:22:45.892980Z","iopub.status.idle":"2023-09-19T10:22:45.919961Z","shell.execute_reply.started":"2023-09-19T10:22:45.892941Z","shell.execute_reply":"2023-09-19T10:22:45.918645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data['smoker'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:46.707349Z","iopub.execute_input":"2023-09-19T10:22:46.707775Z","iopub.status.idle":"2023-09-19T10:22:46.732222Z","shell.execute_reply.started":"2023-09-19T10:22:46.707744Z","shell.execute_reply":"2023-09-19T10:22:46.730799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting textual data of sex and smoker columns to numerical values\n\nraw_data['sex'] = raw_data['sex'].map({'female':0, 'male':1})\nraw_data['smoker'] = raw_data['smoker'].map({'yes':1, 'no':0})","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:47.273661Z","iopub.execute_input":"2023-09-19T10:22:47.274081Z","iopub.status.idle":"2023-09-19T10:22:47.292415Z","shell.execute_reply.started":"2023-09-19T10:22:47.274044Z","shell.execute_reply":"2023-09-19T10:22:47.289678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:47.843316Z","iopub.execute_input":"2023-09-19T10:22:47.844410Z","iopub.status.idle":"2023-09-19T10:22:47.853474Z","shell.execute_reply.started":"2023-09-19T10:22:47.844372Z","shell.execute_reply":"2023-09-19T10:22:47.851797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:48.394967Z","iopub.execute_input":"2023-09-19T10:22:48.395401Z","iopub.status.idle":"2023-09-19T10:22:48.442901Z","shell.execute_reply.started":"2023-09-19T10:22:48.395365Z","shell.execute_reply":"2023-09-19T10:22:48.441387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(raw_data['charges'])\nx_train = np.array(raw_data[['age', 'sex', 'bmi', 'children', 'smoker']])\n\n#Excluding region from the dataset, presently don't know how to factor it in","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:49.003009Z","iopub.execute_input":"2023-09-19T10:22:49.003425Z","iopub.status.idle":"2023-09-19T10:22:49.012976Z","shell.execute_reply.started":"2023-09-19T10:22:49.003391Z","shell.execute_reply":"2023-09-19T10:22:49.011626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing model parameters w and b\n\nb_init = 785.1811367994083\nw_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618, -26.4213161])\nprint(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:49.706456Z","iopub.execute_input":"2023-09-19T10:22:49.706904Z","iopub.status.idle":"2023-09-19T10:22:49.714962Z","shell.execute_reply.started":"2023-09-19T10:22:49.706871Z","shell.execute_reply":"2023-09-19T10:22:49.713033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(x, w, b): \n    \"\"\"\n    single predict using linear regression\n    Args:\n      x (ndarray): Shape (n,) example with multiple features\n      w (ndarray): Shape (n,) model parameters   \n      b (scalar):             model parameter \n      \n    Returns:\n      p (scalar):  prediction\n    \"\"\"\n    p = np.dot(x, w) + b     \n    return p \n\n\ndef compute_cost(X, y, w, b): \n    \"\"\"\n    compute cost\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      cost (scalar): cost\n    \"\"\"\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):                                \n        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n        cost = cost + (f_wb_i - y[i])**2       #scalar\n    cost = cost / (2 * m)                      #scalar    \n    return cost","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:51.166617Z","iopub.execute_input":"2023-09-19T10:22:51.167208Z","iopub.status.idle":"2023-09-19T10:22:51.178361Z","shell.execute_reply.started":"2023-09-19T10:22:51.167165Z","shell.execute_reply":"2023-09-19T10:22:51.176582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Models predicion for a random x: {predict(x_train[0], w_init, b_init)}, corresponding y: {y_train[0]}\")\n\n\nprint(f\"Initial cost for the whole dataset: {compute_cost(x_train, y_train, w_init, b_init)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:52.584698Z","iopub.execute_input":"2023-09-19T10:22:52.585106Z","iopub.status.idle":"2023-09-19T10:22:52.604773Z","shell.execute_reply.started":"2023-09-19T10:22:52.585074Z","shell.execute_reply":"2023-09-19T10:22:52.603222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_gradient(X, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]   \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m                                \n        \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:53.491337Z","iopub.execute_input":"2023-09-19T10:22:53.492438Z","iopub.status.idle":"2023-09-19T10:22:53.500000Z","shell.execute_reply.started":"2023-09-19T10:22:53.492400Z","shell.execute_reply":"2023-09-19T10:22:53.499072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compute and display gradient \ntmp_dj_db, tmp_dj_dw = compute_gradient(x_train, y_train, w_init, b_init)\nprint(f'dj_db at initial w,b: {tmp_dj_db}')\nprint(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:22:54.882681Z","iopub.execute_input":"2023-09-19T10:22:54.883093Z","iopub.status.idle":"2023-09-19T10:22:54.907684Z","shell.execute_reply.started":"2023-09-19T10:22:54.883060Z","shell.execute_reply":"2023-09-19T10:22:54.906467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent to learn w and b. Updates w and b by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X (ndarray (m,n))   : Data, m examples with n features\n      y (ndarray (m,))    : target values\n      w_in (ndarray (n,)) : initial model parameters  \n      b_in (scalar)       : initial model parameter\n      cost_function       : function to compute cost\n      gradient_function   : function to compute the gradient\n      alpha (float)       : Learning rate\n      num_iters (int)     : number of iterations to run gradient descent\n      \n    Returns:\n      w (ndarray (n,)) : Updated values of parameters \n      b (scalar)       : Updated value of parameter \n      \"\"\"\n    \n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               ##None\n        b = b - alpha * dj_db               ##None\n      \n        # Save cost J at each iteration\n        if i<100000:      # prevent resource exhaustion \n            J_history.append( cost_function(X, y, w, b))\n\n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n        \n    return w, b, J_history #return final w,b and J history for graphing","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:23:04.322850Z","iopub.execute_input":"2023-09-19T10:23:04.323255Z","iopub.status.idle":"2023-09-19T10:23:04.334259Z","shell.execute_reply.started":"2023-09-19T10:23:04.323224Z","shell.execute_reply":"2023-09-19T10:23:04.332754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize parameters\ninitial_w = np.zeros_like(w_init)\ninitial_b = 0.\n# some gradient descent settings\niterations = 1000\nalpha = 7.0e-4\n# run gradient descent \nw_final, b_final, J_hist = gradient_descent(x_train, y_train, initial_w, initial_b,\n                                                    compute_cost, compute_gradient, \n                                                    alpha, iterations)\nprint(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\nm,_ = x_train.shape\nfor i in range(10):\n    print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:26:10.515695Z","iopub.execute_input":"2023-09-19T10:26:10.516186Z","iopub.status.idle":"2023-09-19T10:26:27.861469Z","shell.execute_reply.started":"2023-09-19T10:26:10.516148Z","shell.execute_reply":"2023-09-19T10:26:27.860199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Even with 1500 iterations, cost was pretty high, new cost is\ncompute_cost(x_train, y_train, w_final, b_final)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:26:58.330232Z","iopub.execute_input":"2023-09-19T10:26:58.330649Z","iopub.status.idle":"2023-09-19T10:26:58.344817Z","shell.execute_reply.started":"2023-09-19T10:26:58.330617Z","shell.execute_reply":"2023-09-19T10:26:58.343565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets try gradient descent with feature scaling, as the earlier data was taking too long to converge","metadata":{}},{"cell_type":"code","source":"mu     = np.mean(x_train,axis=0)   \nsigma  = np.std(x_train,axis=0) \nx_mean = (x_train - mu)\nx_norm = (x_train - mu)/sigma   ","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:27:08.397161Z","iopub.execute_input":"2023-09-19T10:27:08.397544Z","iopub.status.idle":"2023-09-19T10:27:08.405338Z","shell.execute_reply.started":"2023-09-19T10:27:08.397516Z","shell.execute_reply":"2023-09-19T10:27:08.403995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_w = np.zeros_like(w_init)\ninitial_b = 0.\n# some gradient descent settings\niterations = 1000\nalpha = 7.0e-3\n# run gradient descent \nw_final, b_final, J_hist = gradient_descent(x_norm, y_train, initial_w, initial_b,\n                                                    compute_cost, compute_gradient, \n                                                    alpha, iterations)\nprint(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\nm,_ = x_train.shape\nfor i in range(10):\n    print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:29:18.223446Z","iopub.execute_input":"2023-09-19T10:29:18.223884Z","iopub.status.idle":"2023-09-19T10:29:35.266467Z","shell.execute_reply.started":"2023-09-19T10:29:18.223852Z","shell.execute_reply":"2023-09-19T10:29:35.265145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Even with 1500 iterations, cost was pretty high, new cost is\ncompute_cost(x_norm, y_train, w_final, b_final)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:30:47.502022Z","iopub.execute_input":"2023-09-19T10:30:47.503293Z","iopub.status.idle":"2023-09-19T10:30:47.518805Z","shell.execute_reply.started":"2023-09-19T10:30:47.503243Z","shell.execute_reply":"2023-09-19T10:30:47.517469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By Increasing the conversion rate, we're able to get the algo to converge much faster, and cost came down too. Now lets try n use sciketlearn library for linear regression","metadata":{}},{"cell_type":"markdown","source":"L","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:31:30.557343Z","iopub.execute_input":"2023-09-19T10:31:30.558231Z","iopub.status.idle":"2023-09-19T10:31:31.406959Z","shell.execute_reply.started":"2023-09-19T10:31:30.558191Z","shell.execute_reply":"2023-09-19T10:31:31.405722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nx_norm = scaler.fit_transform(x_train)\nprint(f\"Peak to Peak range by column in Raw        X:{np.ptp(x_train,axis=0)}\")   \nprint(f\"Peak to Peak range by column in Normalized X:{np.ptp(x_norm,axis=0)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:31:31.455160Z","iopub.execute_input":"2023-09-19T10:31:31.455556Z","iopub.status.idle":"2023-09-19T10:31:31.464844Z","shell.execute_reply.started":"2023-09-19T10:31:31.455519Z","shell.execute_reply":"2023-09-19T10:31:31.463462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgdr = SGDRegressor(max_iter=1000)\nsgdr.fit(x_norm, y_train)\nprint(sgdr)\nprint(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:31:32.873668Z","iopub.execute_input":"2023-09-19T10:31:32.874217Z","iopub.status.idle":"2023-09-19T10:31:32.898389Z","shell.execute_reply.started":"2023-09-19T10:31:32.874175Z","shell.execute_reply":"2023-09-19T10:31:32.897327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_norm = sgdr.intercept_\nw_norm = sgdr.coef_\nprint(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")\nprint( \"model parameters from previous lab: w: [ 206.21557815  144.92858396  152.82114471  257.62838369 2533.12885445]\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:32:20.980081Z","iopub.execute_input":"2023-09-19T10:32:20.980514Z","iopub.status.idle":"2023-09-19T10:32:20.987945Z","shell.execute_reply.started":"2023-09-19T10:32:20.980485Z","shell.execute_reply":"2023-09-19T10:32:20.986663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a prediction using sgdr.predict()\ny_pred_sgd = sgdr.predict(x_norm)\n# make a prediction using w,b. \ny_pred = np.dot(x_norm, w_norm) + b_norm  \nprint(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n\nprint(f\"Prediction on training set:\\n{y_pred[:4]}\" )\nprint(f\"Target values \\n{y_train[:4]}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:33:39.578372Z","iopub.execute_input":"2023-09-19T10:33:39.578794Z","iopub.status.idle":"2023-09-19T10:33:39.588807Z","shell.execute_reply.started":"2023-09-19T10:33:39.578764Z","shell.execute_reply":"2023-09-19T10:33:39.587215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot predictions and targets vs original features   \n\nx_features = ['age', 'sex', 'bmi', 'children', 'smoker']\n\nfig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(x_train[:,i],y_train, label = 'target')\n    ax[i].set_xlabel(x_features[i])\n    ax[i].scatter(x_train[:,i],y_pred,color='#FF9300', label = 'predict')\nax[0].set_ylabel(\"Price\"); ax[0].legend();\nfig.suptitle(\"target versus prediction using z-score normalized model\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T10:38:28.696278Z","iopub.execute_input":"2023-09-19T10:38:28.696875Z","iopub.status.idle":"2023-09-19T10:38:29.669670Z","shell.execute_reply.started":"2023-09-19T10:38:28.696825Z","shell.execute_reply":"2023-09-19T10:38:29.668611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}